{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d67FXFqGEIHQ"
   },
   "source": [
    "# **Predict Cancer Mortality Rates in US Counties**\n",
    "\n",
    "The provided dataset comprises data collected from multiple counties in the US. The regression task for this assessment is to predict cancer mortality rates in \"unseen\" US counties, given some training data. The training data ('Training_data.csv') comprises various features/predictors related to socio-economic characteristics, amongst other types of information for specific counties in the country. The corresponding target variables for the training set are provided in a separate CSV file ('Training_data_targets.csv'). Use the notebooks provided for lab sessions throughout this module to provide solutions to the exercises listed below. Throughout all exercises, text describing your code and answering any questions included in the exercise descriptions should be provided as part of your submitted solution. (Total Marks for this Assessment is 40)\n",
    "\n",
    "Note - We also provide an example test data set ('Test_data_example.csv' and 'Test_data_example_targets.csv'). This is just an example of the final test set (which will not be provided to you) that will be used to evaluate your solutions when your submitted solutions are being marked. Part of this assessment requires you to write an inference script that evaluates the regression models you have trained on the final test data set such that we are able to run the inference script ourselves on the test data (you can use the example test data to verify that it works prior to submission).\n",
    "\n",
    "The list of predictors/features available in this data set are described below:\n",
    "\n",
    "**Data Dictionary**\n",
    "\n",
    "avgAnnCount: Mean number of reported cases of cancer diagnosed annually\n",
    "\n",
    "avgDeathsPerYear: Mean number of reported mortalities due to cancer\n",
    "\n",
    "incidenceRate: Mean per capita (100,000) cancer diagoses\n",
    "\n",
    "medianIncome: Median income per county \n",
    "\n",
    "popEst2015: Population of county \n",
    "\n",
    "povertyPercent: Percent of populace in poverty \n",
    "\n",
    "MedianAge: Median age of county residents \n",
    "\n",
    "MedianAgeMale: Median age of male county residents \n",
    "\n",
    "MedianAgeFemale: Median age of female county residents \n",
    "\n",
    "AvgHouseholdSize: Mean household size of county \n",
    "\n",
    "PercentMarried: Percent of county residents who are married \n",
    "\n",
    "PctNoHS18_24: Percent of county residents ages 18-24 highest education attained: less than high school \n",
    "\n",
    "PctHS18_24: Percent of county residents ages 18-24 highest education attained: high school diploma \n",
    "\n",
    "PctSomeCol18_24: Percent of county residents ages 18-24 highest education attained: some college \n",
    "\n",
    "PctBachDeg18_24: Percent of county residents ages 18-24 highest education attained: bachelor's degree \n",
    "\n",
    "PctHS25_Over: Percent of county residents ages 25 and over highest education attained: high school diploma \n",
    "\n",
    "PctBachDeg25_Over: Percent of county residents ages 25 and over highest education attained: bachelor's degree \n",
    "\n",
    "PctEmployed16_Over: Percent of county residents ages 16 and over employed \n",
    "\n",
    "PctUnemployed16_Over: Percent of county residents ages 16 and over unemployed \n",
    "\n",
    "PctPrivateCoverage: Percent of county residents with private health coverage \n",
    "\n",
    "PctPrivateCoverageAlone: Percent of county residents with private health coverage alone (no public assistance) \n",
    "\n",
    "PctEmpPrivCoverage: Percent of county residents with employee-provided private health coverage \n",
    "\n",
    "PctPublicCoverage: Percent of county residents with government-provided health coverage \n",
    "\n",
    "PctPubliceCoverageAlone: Percent of county residents with government-provided health coverage alone \n",
    "\n",
    "PctWhite: Percent of county residents who identify as White \n",
    "\n",
    "PctBlack: Percent of county residents who identify as Black \n",
    "\n",
    "PctAsian: Percent of county residents who identify as Asian \n",
    "\n",
    "PctOtherRace: Percent of county residents who identify in a category which is not White, Black, or Asian \n",
    "\n",
    "PctMarriedHouseholds: Percent of married households \n",
    "\n",
    "BirthRate: Number of live births relative to number of women in county "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kB3aG5f-D4Q4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Define paths to the training data and targets files\n",
    "training_data_path = 'Training_data.csv'\n",
    "training_targets_path = 'Training_data_targets.csv'\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHfmuohnJcc_"
   },
   "source": [
    "# **Exercise 1**\n",
    "\n",
    "Read in the training data and targets files. The training data comprises features/predictors while the targets file comprises the targets (i.e. cancer mortality rates in US counties) you need to train models to predict. Plot histograms of all features to visualise their distributions and identify outliers. Do you notice any unusual values for any of the features? If so comment on these in the text accompanying your code. Compute correlations of all features with the target variable (across the data set) and sort them according the strength of correlations. Which are the top five features with strongest correlations to the targets? Plot these correlations using the scatter matrix plotting function available in pandas and comment on at least two sets of features that show visible correlations to each other. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlTEKBhiM0U5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in training and target files as csv\n",
    "training = pd.read_csv(training_data_path)\n",
    "targets = pd.read_csv(training_targets_path)\n",
    "\n",
    "# histograms for all features \n",
    "training.hist(bins=30, figsize=(20,15))\n",
    "\n",
    "# identify outliers\n",
    "# avgHouseholdSize has a small value which outlies from the rest of the data\n",
    "# by examining the training dataset directly, I also noticed some very \n",
    "# implausible values for the median age variable. There were 26 records where \n",
    "# the median age was more than 300 - this obviously could not be true\n",
    "# small bars, huge whitespace (range extends far), large scale for x axis - outliers exist inthe distribution, but not necessarily implausible e.g. predominantly black areas\n",
    "\n",
    "combined = pd.merge(training, targets, left_index=True, right_index=True)\n",
    "corr_matrix=combined.corr()\n",
    "sortedCorrs = corr_matrix.reindex(corr_matrix.TARGET_deathRate.abs().sort_values(ascending=False).index)[\"TARGET_deathRate\"]\n",
    "print(sortedCorrs)\n",
    "\n",
    "attributes=[\"TARGET_deathRate\",\"PctBachDeg25_Over\",\"incidenceRate\",\"PctPublicCoverageAlone\",\"medIncome\",\"povertyPercent\"]\n",
    "\n",
    "scatter_matrix(combined[attributes],figsize=(12,8))\n",
    "\n",
    "plt.savefig('scatter_matrix.png')\n",
    "\n",
    "# PctPublicCoverageAlone - PovertyPercent\n",
    "# When plotting PovertyPercent against PctPublicCoverageAlone, there is a clear and strong positive correlation between the features.\n",
    "# As the value for PovertyPercent increases i.e., the percentage of the populace in poverty increase, the value for PctPublicCoverageAlone also increases i.e., the percentage of the \n",
    "# population with government funded healthcare also increases. This makes sense intuitively, as you'd expect there to be more\n",
    "# people who require government funding for healthcare in areas of higher poverty.\n",
    "\n",
    "# PctBachDeg25_Over - medIncome\n",
    "# When plotting PctBachDeg25_Over (the percentage of the population in a county aged 25< with a degree) against the medianIncome \n",
    "# in the respective county, there is a clear positive correlation between the two features. This also makes sense intuitively, as you'd \n",
    "# expect the average salary to increase given that someone has a degree and hence, where there is a higher percentage of people\n",
    "# with degrees, you find a higher median income value for that county. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for my own interest, to see how other features correlate with each other\n",
    "attributes=[\"TARGET_deathRate\",\"incidenceRate\",\"PctPublicCoverageAlone\",\"medIncome\",\"povertyPercent\", \"avgAnnCount\", \"avgDeathsPerYear\", \"popEst2015\"]\n",
    "\n",
    "scatter_matrix(combined[attributes],figsize=(12,8), alpha=0.3)\n",
    "plt.savefig('scatter_matrix_full.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc ranges\n",
    "columns = combined.columns.values.tolist()\n",
    "\n",
    "for column in columns:\n",
    "  range = combined[column].max() - combined[column].min()\n",
    "  print(f\"column: {column}, range: {range}\")\n",
    "\n",
    "# median age range shows there are outliers in median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show maximums\n",
    "columns = combined.columns.values.tolist()\n",
    "\n",
    "for column in columns:\n",
    "  max = combined[column].max()\n",
    "  print(f\"column: {column}, max: {max}\")\n",
    "\n",
    "# median age definitely has outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wSA10GzM_Xu"
   },
   "source": [
    "# **Exercise 2**\n",
    "\n",
    "Create an ML pipeline using scikit-learn (as demonstrated in the lab notebooks) to pre-process the training data. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with unrealistic median age values and avgHouseholdSize values\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "missing_columns = ['PctSomeCol18_24', 'PctEmployed16_Over', 'PctPrivateCoverageAlone']\n",
    "\n",
    "class RemoveImplausibleValues(BaseEstimator):\n",
    "    \n",
    "    def fit(self,X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        x = X.copy(deep=True)\n",
    "        # x = x[~((x['MedianAge'] > 300) | (x['AvgHouseholdSize'] <1))]\n",
    "        # for column in x.columns:\n",
    "        #     if x[column].isnull().sum() > 0:\n",
    "        #         print(column)\n",
    "        #         missing_columns.append(column)\n",
    "        #         x.drop(column, axis=1, inplace=True)\n",
    "        x.drop(missing_columns, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# print(len(combined))\n",
    "# combined = combined[~((combined['MedianAge'] > 300) | (combined['AvgHouseholdSize'] <1))]\n",
    "# print(len(combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before splitting the data, remove records with implausible values\n",
    "# create scikit learn pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "initial_pipeline = Pipeline([\n",
    "    ('RemoveImplausibleValues',RemoveImplausibleValues())\n",
    "])\n",
    "\n",
    "print(len(combined))\n",
    "combined_ppln = initial_pipeline.fit_transform(combined).reset_index(drop=True)\n",
    "# combined_ppln[\"TARGET_deathRate\"].head()\n",
    "print(len(combined_ppln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_ppln.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "def stratified_split(df, feature):\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "    \n",
    "    # returns 2 sets of indexes for test and train\n",
    "    # hence, .loc is used on the dataset df to retrieve the corresponding records\n",
    "    for train_index, test_index in split.split(df,df[feature]):\n",
    "        strat_train_set = df.loc[train_index]\n",
    "        strat_test_set = df.loc[test_index]\n",
    "    \n",
    "    for set_ in (strat_train_set, strat_test_set):\n",
    "        set_.drop((feature),axis=1,inplace=True)\n",
    "        \n",
    "   \n",
    "    return strat_train_set, strat_test_set\n",
    "\n",
    "\n",
    "def shuffle_split(df, target):\n",
    "    split = ShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "    \n",
    "    # returns 2 sets of indexes for test and train\n",
    "    # hence, .loc is used on the dataset df to retrieve the corresponding records\n",
    "    for train_index, test_index in split.split(df,df[target]):\n",
    "        train_set = df.loc[train_index]\n",
    "        test_set = df.loc[test_index]\n",
    "    \n",
    "#     for set_ in (strat_train_set, strat_test_set):\n",
    "#         set_.drop((target),axis=1,inplace=True)\n",
    "        \n",
    "   \n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "train_set, test_set = shuffle_split(combined_ppln, 'TARGET_deathRate')\n",
    "\n",
    "training=train_set.drop(\"TARGET_deathRate\",axis=1)\n",
    "training_labels=train_set[\"TARGET_deathRate\"].copy()\n",
    "\n",
    "testing = test_set.drop(\"TARGET_deathRate\",axis=1)\n",
    "testing_labels= test_set[\"TARGET_deathRate\"].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training), len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.info()\n",
    "# NOTE: PctSomeCol18_24, PctEmployed16_Over,PctPrivateCoverageAlone all contain \n",
    "# less than 2438 non-null values i.e. have null values which require imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems some columns (PctSomeCol18_24, PctEmployed16_Over, PctPrivateCoverageAlone) have null values - \n",
    "# let's validate this\n",
    "# print(combined['PctSomeCol18_24'].isnull().any())\n",
    "# print(combined['PctEmployed16_Over'].isnull().any())\n",
    "# print(combined['PctPrivateCoverageAlone'].isnull().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JJ21KooNRVT"
   },
   "outputs": [],
   "source": [
    "# impute - currently using median!\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer=SimpleImputer(strategy=\"median\")\n",
    "# # fit\n",
    "# imputer.fit(combined)\n",
    "# # array of size 32, with median value of each column\n",
    "# imputer.statistics_ \n",
    "# # transform\n",
    "# x = imputer.transform(combined)\n",
    "# x.shape\n",
    "# # turn np.array back into dataframe\n",
    "# combined = pd.DataFrame(x,columns=combined.columns)\n",
    "# combined.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q to self: IS MEDIAN THE BEST CHOICE TO IMPUTE WITH? TRY WITH MEAN AND SEE WHICH YIELDS BETTER RESULTS!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate and add features\n",
    "\n",
    "\n",
    "# normalise - make values 0 to 1 - used for neural networks\n",
    "\n",
    "\n",
    "# Standardise - minus mean, divide by variance\n",
    "\n",
    "\n",
    "# no encoding needed e.g. one hot encoding, since all variables are numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scikit learn pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# could try iterative imputer\n",
    "pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "training_ppln = pipeline.fit_transform(training)\n",
    "print(len(training_ppln))\n",
    "# type(combined_ppln), combined.columns\n",
    "# turn it back into a df\n",
    "# training_ppln = pd.DataFrame(data=training_ppln, columns=training.columns)\n",
    "# print(training_ppln.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ppln_df = pd.DataFrame(data=training_ppln, columns=training.columns)\n",
    "training_ppln_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiM5kym6NRww"
   },
   "source": [
    "# **Exercise 3**\n",
    "\n",
    "Fit linear regression models to the pre-processed data using: Ordinary least squares (OLS), Lasso and Ridge models. Choose suitable regularisation weights for Lasso and Ridge regression and include a description in text of how they were chosen. In your submitted solution make sure you set the values for the regularisation weights equal to those you identify from your experiment(s). Quantitatively compare your results from all three models and report the best performing one. Include code for all steps above. (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use combined df\n",
    "# split train test sets 80:20 - use stratified sampling based on a feature/variable?\n",
    "# do linear regression on training set with cross validation \n",
    "# get metrics from cross validation and aggregate results (E.g. avg)\n",
    "# to measure how effective selected hyperparameters are\n",
    "# use gridsearchCV to automatically find the best hyper parameters for model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data=testing\n",
    "some_labels=testing_labels\n",
    "some_data_prepared=pipeline.transform(some_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv9wFbldPo45"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "#OLS - Ordinary Least Squares\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(training_ppln, training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted = lin_reg.predict(some_data_prepared)\n",
    "# print(\"Predictions:\", predicted)\n",
    "# print(\"Labels:\", list(some_labels))\n",
    "mse = mean_squared_error(list(some_labels), predicted)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(list(some_labels), predicted)\n",
    "r2 = r2_score(list(some_labels), predicted)\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"mae: {mae}\")\n",
    "print(f\"r2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lasso = Lasso()\n",
    "\n",
    "#regularisation strength - alpha\n",
    "param_grid = {'alpha': np.arange(1,100,1)}\n",
    "\n",
    "\n",
    "# Create a GridSearchCV object - using 5 folds, could try 10 folds too or more\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model to the data using the grid search object\n",
    "grid_search.fit(training_ppln, training_labels)\n",
    "\n",
    "# Get the best regularization weight\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BEST LASSO REGULARISATION WEIGHT:***\n",
    "The best value of alpha found for Lasso was 0.0415. I found this value for alpha by starting with a relatively large range of values for alpha (0.001 to 10) and iteratively narrowing down the range to  converge on a more optimal value for alpha.\n",
    "The best values found for alpha after each iteration were \\[0.01, 0.05, 0.05, 0.04, 0.04, 0.04, 0.04,0.042, 0.041, 0.0415\\]\n",
    "\n",
    "So, the value I decided to use was 0.0415 for alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.0415).fit(training_ppln, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lasso.predict(some_data_prepared)\n",
    "# print(\"Predictions:\", predicted)\n",
    "# print(\"Labels:\", list(some_labels))\n",
    "mse = mean_squared_error(list(some_labels), predicted)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(list(some_labels), predicted)\n",
    "r2 = r2_score(list(some_labels), predicted)\n",
    "\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"mae: {mae}\")\n",
    "print(f\"r2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "\n",
    "#regularisation strength - alpha\n",
    "param_grid = {'alpha': [13.34, 13.35, 13.6]}\n",
    "\n",
    "\n",
    "# Create a GridSearchCV object - using 5 folds, could try 10 folds too or more\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model to the data using the grid search object\n",
    "grid_search.fit(training_ppln, training_labels)\n",
    "\n",
    "# Get the best regularization weight\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BEST RIDGE REGULARISATION WEIGHT:***\n",
    "The best value of alpha found for Lasso was 13.35. I found this value for alpha by starting with a relatively large range of values for alpha (0.001 to 10) and iteratively narrowing down the range to  converge on a more optimal value for alpha.\n",
    "The best values found for alpha after each iteration were \\[10, 10, 15, 12.5, 13.75, 13, 13.5, 13.25, 13.4, 13.35, 13.35, 13.35\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=13.35).fit(training_ppln, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = ridge.predict(some_data_prepared)\n",
    "# print(\"Predictions:\", predicted)\n",
    "# print(\"Labels:\", list(some_labels))\n",
    "mse = mean_squared_error(list(some_labels), predicted)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(list(some_labels), predicted)\n",
    "r2 = r2_score(list(some_labels), predicted)\n",
    "\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"mae: {mae}\")\n",
    "print(f\"r2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdx4FmjqPpVJ"
   },
   "source": [
    "# **Exercise 4**\n",
    "\n",
    "Use Lasso regression and the best regularisation weight identified from Exercise 3 to identify the five most important/relevant features for the provided data set and regression task. Report what these are desceding order of their importance. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vC63VPnQXI_"
   },
   "outputs": [],
   "source": [
    "weights = np.abs(lasso.coef_)\n",
    "\n",
    "# sortedCorrs = corr_matrix.reindex(corr_matrix.TARGET_deathRate.abs().sort_values(ascending=False).index)[\"TARGET_deathRate\"]\n",
    "\n",
    "features_weights = sorted(dict(zip(training_ppln_df.columns, weights)).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "feature_weights = pd.DataFrame(features_weights[:5], columns=['Feature', 'Weight'])\n",
    "\n",
    "display(feature_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQvZqdqGQ_pH"
   },
   "source": [
    "# **Exercise 5**\n",
    "\n",
    "Fit a Random Forest regression model to the training data and quantitatively evaluate and compare the Random Forest regression model with the best linear regression model identified from Exercise 3. Report which model provides the best results. Next, report the top five most important/relevant features for the provided data set and regression task identified using the Random Forest model. Comment on how these compare with the features identified from Lasso regression? (14 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbFXu6UiQ-gv"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(random_state=42)\n",
    "parameters = {\n",
    "  'n_estimators': [100],\n",
    "  'max_depth': [10],\n",
    "  \"random_state\": [21],\n",
    "  \"max_features\" : [30]\n",
    "}\n",
    "\n",
    "forest = GridSearchCV(forest, parameters, scoring='neg_mean_squared_error', cv=10)\n",
    "forest.fit(training, np.ravel(training_labels))\n",
    "forest_predictions = forest.predict(testing)\n",
    "forest = forest.best_estimator_\n",
    "print(f\"forest: {forest}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics RMSE, MSE, MAE, R2\n",
    "# predicted = forest.predict(some_data_prepared)\n",
    "# print(\"Predictions:\", predicted)\n",
    "# print(\"Labels:\", list(some_labels))\n",
    "mse = mean_squared_error(list(some_labels), forest_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(list(some_labels), forest_predictions)\n",
    "r2 = r2_score(list(some_labels), predicted)\n",
    "\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"mae: {mae}\")\n",
    "print(f\"r2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with best linear one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 5 features from RF\n",
    "relevance = forest.feature_importances_\n",
    "features_weights = sorted(dict(zip(training_ppln_df.columns, relevance)).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "feature_weights = pd.DataFrame(features_weights[:5], columns=['Feature', 'Weight'])\n",
    "\n",
    "display(feature_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg-Tksx5QXiL"
   },
   "source": [
    "# **Exercise 6**\n",
    "\n",
    "Use the provided test example data ('Test_data_example.csv' and 'Test_data_example_targets.csv') to write an inference script to evaluate the best regression model identified from preceding exercises. First re-train the chosen regression model using all of the provided training data and test your predictions on the provided example test data. Note - the final evaluation of your submission will be done by replacing this example test data with held out (unseen) test data that is not provided to you. But the format of this \"unseen\" test data will be identical to the example test data provided to you. Use the code snippet provided below to prepare your inference script to predict targets for the unseen test data. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcmuUJMODD2Q"
   },
   "outputs": [],
   "source": [
    "## Read in the provided example test data\n",
    "test_data_path = 'Test_data_example.csv'\n",
    "test_targets_path ='Test_data_example_targets.csv'\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_targets = pd.read_csv(test_targets_path)\n",
    "## Retrain your chosen regression model here \n",
    "# For example: lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X_train,y_train) where X_train and y_train is provided training data\n",
    "# Next write the lines of code required to predict on unseen test data and evaluate your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_new = pd.read_csv(training_data_path)\n",
    "\n",
    "# training_new_labels=combined_ppln[\"TARGET_deathRate\"].copy()\n",
    "# training_new=combined_ppln.drop(\"TARGET_deathRate\",axis=1, inplace=True)\n",
    "\n",
    "train_data_ppln = pipeline.fit_transform(combined_ppln)\n",
    "\n",
    "test_data_ppln = initial_pipeline.fit_transform(test_data)\n",
    "test_data_ppln = pipeline.fit_transform(test_data_ppln)\n",
    "\n",
    "lasso_final = Lasso(alpha=0.0415)\n",
    "lasso_final.fit(train_data_ppln, training_new_labels)\n",
    "print(len(train_data_ppln[0]), len(training_new_labels))\n",
    "\n",
    "print(len(test_data_ppln[0]))\n",
    "final_predictions = lasso_final.predict(test_data_ppln)\n",
    "\n",
    "print(len(test_targets), len(final_predictions))\n",
    "\n",
    "mse = mean_squared_error((test_targets), final_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error((test_targets), final_predictions)\n",
    "r2 = r2_score((test_targets), final_predictions)\n",
    "\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"mae: {mae}\")\n",
    "print(f\"r2: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5611M-Coursework Assessment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e85b9385b78cc0f4bde3ac16ced38f32c4bb331e413e9d9541fc5ff5fa933235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
