{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">COMP3611 - Building a Machine Learning Pipeline (part 2)</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Marc de Kamps and University of Leeds</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a machine learning (part 2)\n",
    "\n",
    "### Objectives\n",
    "\n",
    "We will clean the data, in particular, we will\n",
    "- Impute missing values\n",
    "- Replace alpha-numeric information by one-hot-encoded vectors to arrive at a purely numerical implementation\n",
    "- Add features to the dataset that are likely to boost prediction power\n",
    "- We will use Transformer and Pipeline objects to provide self-documenting implementations of these steps\n",
    "\n",
    "Note that we have left the sample answers in place, or you wouldn't be able to run through the notebook.\n",
    "\n",
    "### Caveat\n",
    "\n",
    "There is one small error in this notebook that we will leave in place, but have removed in (Part 3). Once you have run (Part 3), see if you can discover it.\n",
    "**Warning** Use Part 3 as a basis for further coding, not this notebook (part 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start where we left off in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "local_path = 'datasets/housing'\n",
    "\n",
    "\n",
    "def restore():\n",
    "    housing_tgz=tarfile.open(os.path.join(local_path,'./housing.tgz'))\n",
    "    housing_tgz.extractall(path=local_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "    csv_path=os.path.join(local_path,'./housing.csv')\n",
    "    housing = pd.read_csv(csv_path)\n",
    "\n",
    "    # create test training set with stratified sampling (see previous notebook)\n",
    "    housing[\"income_category\"]=np.ceil(housing[\"median_income\"]/1.5)\n",
    "    housing[\"income_category\"].where(housing[\"income_category\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "    \n",
    "    for train_index, test_index in split.split(housing,housing[\"income_category\"]):\n",
    "        strat_train_set = housing.loc[train_index]\n",
    "        strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "    for set_ in (strat_train_set, strat_test_set):\n",
    "        set_.drop((\"income_category\"),axis=1,inplace=True)\n",
    "        \n",
    "\n",
    "    # this is the mistake - no need to remove the labels from dataset\n",
    "    housing.drop((\"income_category\"),axis=1,inplace=True)\n",
    "\n",
    "   \n",
    "    return housing, strat_train_set, strat_test_set\n",
    "\n",
    "housing, strat_train_set, start_test_set = restore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "In many real world data sets there is missing data. Doctors forget to register the temperature at some days, sensors are sometimes faulty and produce no sensible output, etc. Pandas is a considerable step up from comma separated files in that they have an explicit representation for a missing value: *NaN*.\n",
    "\n",
    "In general, machine learning algorithms can't work with NaN, however and this means either removing data where NaN occurs or replacing them by a sensible default value, a process called *imputation*.\n",
    "\n",
    "Here you have three choices:\n",
    "-Drop districts where NaN occurs\n",
    "-Drop the entire attribute (i.e. remove the entire column where NaN occurs)\n",
    "-Impute\n",
    "\n",
    "Many imputation strategies are simple: they replace the missing value by the median (or sometimes mean) of the attribute. This is potentially dangerous: if the pattern of missingness is not random, but has systematic causes it may be wrong to use this strategy. More sophisticated strategies, that we will not consider here, consist of using for example a random forest or decision tree to try and predict the missing values.\n",
    "\n",
    "Pandas offers quick ways for implementing simple imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         129.0\n",
       "1        1106.0\n",
       "2         190.0\n",
       "3         235.0\n",
       "4         280.0\n",
       "          ...  \n",
       "20635     374.0\n",
       "20636     150.0\n",
       "20637     485.0\n",
       "20638     409.0\n",
       "20639     616.0\n",
       "Name: total_bedrooms, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#housing.dropna(subset=[\"total_bedrooms\"]) # drop those districts for which the total_bedrooms value is missing\n",
    "#housing.drop(\"total_bedrooms\",axis=1) # drop the entire column\n",
    "\n",
    "# impute missing values by the median\n",
    "median=housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*scikit-learn* itself also offers support for simple imputation strategies. And since this nicely dovetails with the use of pipelines, which is an important topic in this notebook, let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer=SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be as simple as calling the fit method on the data frame. So:\n",
    "\n",
    "            imputer.fit(housing)\n",
    "\n",
    "You will find this will cause an exception to be thrown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'NEAR BAY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhousing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:364\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    356\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter was deprecated in version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1 and will be removed in 1.3. A warning will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    362\u001b[0m     )\n\u001b[1;32m--> 364\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:317\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[0;32m    312\u001b[0m     new_ve \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    314\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, ve\n\u001b[0;32m    315\u001b[0m         )\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use median strategy with non-numeric data:\ncould not convert string to float: 'NEAR BAY'"
     ]
    }
   ],
   "source": [
    "imputer.fit(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Examine the data frame to see what causes this problem.\n",
    "\n",
    "*ANSWER*: SimpleImputer is calculating the median value for each column, however it cannot do this on columns which are not completely numeric - hence, an error is thrown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** The offending data itself does not need to be imputed. So it could be dropped, at least temporary, from the dataframe. Research how you can create a dataframe that doesn't contain the offending data, then fit the imputer to it. Once you have done this, print the\n",
    "\n",
    "                imputer.statistics_\n",
    "                \n",
    "member variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0    -122.23     37.88                41.0        880.0           129.0   \n",
      "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
      "2    -122.24     37.85                52.0       1467.0           190.0   \n",
      "3    -122.25     37.85                52.0       1274.0           235.0   \n",
      "4    -122.25     37.85                52.0       1627.0           280.0   \n",
      "\n",
      "   population  households  median_income  median_house_value ocean_proximity  \n",
      "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
      "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
      "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
      "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
      "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
      "0        NEAR BAY\n",
      "1        NEAR BAY\n",
      "2        NEAR BAY\n",
      "3        NEAR BAY\n",
      "4        NEAR BAY\n",
      "           ...   \n",
      "20635      INLAND\n",
      "20636      INLAND\n",
      "20637      INLAND\n",
      "20638      INLAND\n",
      "20639      INLAND\n",
      "Name: ocean_proximity, Length: 20640, dtype: object\n",
      "[-1.1849e+02  3.4260e+01  2.9000e+01  2.1270e+03  4.3500e+02  1.1660e+03\n",
      "  4.0900e+02  3.5348e+00  1.7970e+05]\n"
     ]
    }
   ],
   "source": [
    "#! Sample answer\n",
    "print(housing.head())\n",
    "dropped = housing['ocean_proximity']\n",
    "print(dropped)\n",
    "housing_num =housing.drop(\"ocean_proximity\",axis=1)\n",
    "imputer.fit(housing_num)\n",
    "print(imputer.statistics_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous question, you should have created a new dataframe that doesn't contain the 'ocean_proximity' attribute. Let's assume this reduced dataframe is called *housing_num* which now is guaranteed to contain numerical data only.\n",
    "\n",
    "The fit method has not changed *housing_num* at all. It only has fed the data of the reduced dataframe to the imputer and allowed it to calculate median values for each of the columns. This is called *training* the imputer.\n",
    "Now the imputer must be used to actually clean up the data of the *housing_num* frame. This is done by calling the imputer's *transform* method on *housing_num*. **Before proceeding, please ensure you have created the reduced dataframe and called it housing_num or the notebook will crash. Such a crash is not acceptable in your coursework submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Examine the form of the data represented by X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2223e+02  3.7880e+01  4.1000e+01  8.8000e+02  1.2900e+02  3.2200e+02\n",
      "  1.2600e+02  8.3252e+00  4.5260e+05]\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "#! Sample answer\n",
    "print(X[0])\n",
    "print(X.shape)\n",
    "\n",
    "# So just one big numpy array, but without Nan!\n",
    "# (20640, 9) means this array of arrays has 20640 indexes i.e. arrays\n",
    "# and each array contains 9 values i.e. a data value for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** Convert X back into a dataframe with the original column names (minus *ocean_proximity*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  \n",
       "2       496.0       177.0         7.2574            352100.0  \n",
       "3       558.0       219.0         5.6431            341300.0  \n",
       "4       565.0       259.0         3.8462            342200.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Sample answer\n",
    "housing_tr = pd.DataFrame(X,columns=housing_num.columns)\n",
    "housing_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and Categorical Variables\n",
    "\n",
    "In general machine learning algorithms need numerical values to work and *scikit-learn* expects data in two dimensional numpy arrays (like X) in the previous example. It is therefore necessary to convert text which often indicates a categorical variable (like, '<1H OCEAN', 'INLAND', 'NEAR OCEAN' etc.) into numerical values. (An exception to this would be a project that tries to apply NLP to text fragments, but that is not the case here; the column\n",
    "*ocean_proximity* clearly contains categories). It would be possible to convert  them into numerical values as in '<1H OCEAN' = 0, 'INLAND' = 1, etc.. There is a potential problem with this as there is an impcit concept of nearness in this coding: 0 and 1 are closer together than 1 and 4, for example. If used in a clustering algorithm this nearness may be used without this being intended by the investigator. A one-hot encoding does not have this problem.\n",
    "\n",
    "A one-hot coding would code 5 categories as follows:\n",
    "(1,0,0,0,0)\n",
    "(0,1,0,0,0)\n",
    "   ...\n",
    "(0,0,0,0,1)\n",
    "\n",
    "There is no unintended concept of proximity in this coding and it is to be preferred for coding categorical variables numerically.\n",
    "\n",
    "Again *scikit-learn* offers a custom-made class for transforming textual categorical variables into one-hot encoded ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "      dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder()\n",
    "# reshape (row, column) says to reshape an array such that it has row rows and column columns\n",
    "# -1 can be used as one of the parameters though, and it basically lets np calculate the corresponding value for either no.rows\n",
    "# or no.columns. If theres a 3*4 array and reshape (6,-1) is passed, there will be 6 rows and the no.columns is calculated via\n",
    "# (3*4) / 6 = 2. hence, 2 columns in the reshaped array\n",
    "enc.fit(housing[\"ocean_proximity\"].values.reshape(-1,1))\n",
    "print(enc.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*reshape*  is necessary because a transformer (such as Imputer or OneHotEncoder) expects a 2D numpy array. **housing[\"ocean_proximity\"]** is a *pandas.Series*, hence the use of values. The -1 is a neat little syntactic trick to prevent having to know or calculate the length of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform(housing[\"ocean_proximity\"].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "*Imputer* and *OneHotEncoder* are so-called **transformers**. Whenever you start to realise that you need to do a lot of repetitive processing, you should either use a tranformer or design one of your own, in case *scikit-learn*  doesn't have one that is necessary for your processing.\n",
    "\n",
    "You can design a class of your own that encapsulates your preprocessing code. All you need to do is provide functions: \n",
    "\n",
    "                fit()\n",
    "                transform()\n",
    "                fit_transform()\n",
    "                \n",
    "*fit_transform* simply first calls *fit* and then *transform*\n",
    "\n",
    "As an example, lets add some preprocessing that adds the number of rooms per house hold and optionally also\n",
    "the number of rooms per household. Earlier we have seen that the number rooms per house hold correlates quite strongly with the median house value and it may be a variable that we want to represent explicitly in the dataset, meaning that we have to transform it. Optionally, we may want the number of bedrooms per room to the dataset as well. We will make the transformer configurable so that later in your analysis pipeline you can investigate both options, simply by setting a binary flag, which then becomes a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-122.23 37.88 41.0 ... 6.984126984126984 0.3659090909090909\n",
      "  0.14659090909090908]\n",
      " [-122.22 37.86 21.0 ... 6.238137082601054 0.3382166502324271\n",
      "  0.15579659106916466]\n",
      " [-122.24 37.85 52.0 ... 8.288135593220339 0.338104976141786\n",
      "  0.12951601908657123]\n",
      " ...\n",
      " [-121.22 39.43 17.0 ... 5.20554272517321 0.44676131322094054\n",
      "  0.21517302573203195]\n",
      " [-121.32 39.43 18.0 ... 5.329512893982808 0.39838709677419354\n",
      "  0.21989247311827956]\n",
      " [-121.24 39.37 16.0 ... 5.254716981132075 0.4980251346499102\n",
      "  0.22118491921005387]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator):\n",
    "\n",
    "    def __init__(self, do_add_bedrooms_per_room = False):\n",
    "        \n",
    "        # simply a binary variable per room\n",
    "        self.do_add_bedrooms_per_room = do_add_bedrooms_per_room\n",
    "        \n",
    "        # These are the column indices of the respective columns. OK for illustration purposes.\n",
    "        # For more robust code you would want to extract these values from the DataFrame by name.\n",
    "        self.rooms_ix      = 3\n",
    "        self.bedrooms_ix   = 4\n",
    "        self.population_ix = 5\n",
    "        self.household_ix  = 6\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # We don't transform the target values here\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:,self.rooms_ix]/X[:,self.household_ix]\n",
    "        population_per_household = X[:, self.population_ix]/ X[:,self.rooms_ix]\n",
    "        if self.do_add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,self.bedrooms_ix]/X[:,self.rooms_ix]\n",
    "            return np.c_[X,rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_adder=CombinedAttributesAdder(do_add_bedrooms_per_room=True)\n",
    "housing_extra_attribs=attr_adder.transform(housing.values)\n",
    "print(housing_extra_attribs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My questions:**\n",
    "1. What is a hyperparameter?\n",
    "1a. What is the difference between hyperparam and feature and parameter?\n",
    "A feature is a column in the ds. Parameters can be seen as the weights applied to the features in a regression model, for example, and a hyperparameter is a parameter used to tune the modelling algorithm itself e.g. number of trees in a random forest\n",
    "\n",
    "2. What is the point of the fit function here?\n",
    "Fit calculates a value for every column - median in this case\n",
    "\n",
    "3. What does the notation with the colons etc mean when used in the transform method on X?\n",
    "It does python slicing - so instead of saying get me this element at ith index, it says just bring me back the whole column / every value from the column\n",
    "\n",
    "4. What is np.c_ doing?\n",
    "just stacks vectors - as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra attributes have now been added to the data, which are spit out as a 2D numpy array without further structure.  Ultimately, this is what you need if you want to do numerical processing and exactly what *scikit-learn* algorithms expect.\n",
    "\n",
    "The *np.c_* is a numpy shorthand for grouping vectors into matrices. There are many ways of achieving the same effect, some people prefer *hstack* , *vstack* constructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3]).T\n",
    "b=np.array([4,5,6]).T\n",
    "print(np.c_[a,b])\n",
    "\n",
    "print(np.vstack([a,b]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Most machine learning algorithms perform better if the numerical values of the attributes are of comparable magnitude. The total number of rooms varies between 0 and 39320, whereas income is between 0 and 15. Typically, the attributes - but not the target variable, here median house value - are rescaled. An exception are decision tree based methods where the data is taken as is.\n",
    "\n",
    "#### Min-max scaling (normalisation)\n",
    "\n",
    "The data are shifted and rescaled so that their minimum value corresponds to 0 and their maximum value corresponds to 1. *scikit-learn* provides the **MinMaxScaler** to achieve this. It can be recongured to other target ranges than $[0,1]$ where desired.\n",
    "\n",
    "#### Standardisation\n",
    "The mean is substracted of all data points and then divided by the variance so that a zero-mean unit variance distribution results.\n",
    "\n",
    "There are pros and cons to each of these methods. Some machine learning algorithms (neural networks) expect input values to be within a certain range, meaning that you have to use min-max scaling. But a single outlier can compress the other data points to heap up around 0, something that doesn't happen to the same extent in standardisation.\n",
    "\n",
    "*scikit-learn* provides the *StandardScalar* transformer to implement this method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipelines group transformers sequentially and provide a single access point for running the entire preprocessing chain. The transformations we have discussed so far crop up almost in every machine learning analysis. Each transformation step is usually simple, and it may be that initially you have cobbled an ad hoc preprocessing step to get going with analysis. Such code tends to obscure your programmes. If you find yourself bored because you're constantly writing relatively simple proeprocessing steps, you're not using pipelines properly. The individual\n",
    "Transformers can be hidden in python modules that you can import. The Pipelines define a very short definition\n",
    "of the preprocessing steps and are almost self-documenting. An example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder',CombinedAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is very readable, moreover since imputing and scaling occur so often, similar pipelines can\n",
    "often be constructed for other analyses, and the indvidual components can be re-used with a minimum of new programming effors. \n",
    "\n",
    "The names are practical for documentation, but also allow named access to the elements of the pipeline in case they're needed downstream in the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SimpleImputer(strategy='median')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pipeline['imputer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the pipeline *fit* method, it calls the *fit_transform* method of the elements of the pipeline in order. All but the last elements of the pipeline must be *transformers*, the last can be an *estimator* (an *estimator*) has *fit* method, but not necessarily a *transform* or *fit_transform* method. )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What is an estimator and what actually does fit do?\n",
    "Fit calculate values, such as fit(median) will calculate the median for each column, and then transform will apply these values to the NaN values.\n",
    "An estimator just collects data e.g. by using fit, but doesn't tranform the data in any way. It collects data and outputs it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Seperate Pipelines for Numerical and Categorical Values\n",
    "\n",
    "We have seen that we need to handle numerical and categorical values differently. This effectively boils down to the creation of two pipelines that each run on mutually exclusive columns. We would like to run these pipelines in parallel and combine their joint output in one 2D numpy array which contains the conversion of all attributes to sensible numerical values.\n",
    "\n",
    "To this end we will write a custom transformer that selects only certain columns of the dataset, and drops the other ones. We will use that transformer, called **DataFrameSelector** to create two parallel pipelines and then\n",
    "combined the two resulting datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names= attribute_names\n",
    "        \n",
    "    def fit(self,X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although there appears to be no pandas code, this works because X is a panda DataFrame, which can take a list of column names and select the appropriate columns!\n",
    "\n",
    "The two pipelines can now be created as follows **from the training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs=list(housing) # using pandas to get the names of attributes in housing_num, which we made by dropping ocean_proximity\n",
    "num_attribs.remove(\"ocean_proximity\")\n",
    "cat_attribs=[\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline= Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder',CombinedAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_attribs)),\n",
    "    ('one hot',OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These pipelines can be run independently ('in parallel'). Their results now need to be combined. *scikit-learn* offers the **FeatureUnion** for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\",num_pipeline),\n",
    "    (\"cat_pipeline\",cat_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole pipeline can now be run in one go on the original *housing* DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  (0, 0)\t-1.3278352216308462\n",
      "  (0, 1)\t1.0525482830366848\n",
      "  (0, 2)\t0.9821426581785077\n",
      "  (0, 3)\t-0.8048190966246049\n",
      "  (0, 4)\t-0.9724764790070289\n",
      "  (0, 5)\t-0.9744285971768408\n",
      "  (0, 6)\t-0.9770328537634586\n",
      "  (0, 7)\t2.3447657583017163\n",
      "  (0, 8)\t2.129631481668038\n",
      "  (0, 9)\t0.6285594533305325\n",
      "  (0, 10)\t-0.08762704370782531\n",
      "  (0, 14)\t1.0\n",
      "  (1, 0)\t-1.3228439144608317\n",
      "  (1, 1)\t1.043184551645693\n",
      "  (1, 2)\t-0.6070189133741593\n",
      "  (1, 3)\t2.045890099248613\n",
      "  (1, 4)\t1.357143430032187\n",
      "  (1, 5)\t0.8614388682720688\n",
      "  (1, 6)\t1.6699610275640624\n",
      "  (1, 7)\t2.3322379635373314\n",
      "  (1, 8)\t1.314156136924335\n",
      "  (1, 9)\t0.32704135754480507\n",
      "  (1, 10)\t-0.0971931712397134\n",
      "  (1, 14)\t1.0\n",
      "  (2, 0)\t-1.3328265288008536\n",
      "  :\t:\n",
      "  (20637, 12)\t1.0\n",
      "  (20638, 0)\t-0.8736262691597478\n",
      "  (20638, 1)\t1.7782374658384243\n",
      "  (20638, 2)\t-0.8453931491070593\n",
      "  (20638, 3)\t-0.35559976683593253\n",
      "  (20638, 4)\t-0.3048269656692802\n",
      "  (20638, 5)\t-0.6044293340584699\n",
      "  (20638, 6)\t-0.3937525814946471\n",
      "  (20638, 7)\t-1.0545829218829477\n",
      "  (20638, 8)\t-1.058608468716747\n",
      "  (20638, 9)\t-0.04021111407260373\n",
      "  (20638, 10)\t-0.07640778175419059\n",
      "  (20638, 12)\t1.0\n",
      "  (20639, 0)\t-0.8336958117996527\n",
      "  (20639, 1)\t1.7501462716654528\n",
      "  (20639, 2)\t-1.004309306262326\n",
      "  (20639, 3)\t0.06840827403602866\n",
      "  (20639, 4)\t0.18875678169112686\n",
      "  (20639, 5)\t-0.03397700953706748\n",
      "  (20639, 6)\t0.079672213485868\n",
      "  (20639, 7)\t-0.7801294685152976\n",
      "  (20639, 8)\t-1.0178780317316736\n",
      "  (20639, 9)\t-0.07044251689612338\n",
      "  (20639, 10)\t-0.04198863508253544\n",
      "  (20639, 12)\t1.0\n"
     ]
    }
   ],
   "source": [
    "housing.head()\n",
    "#housing=strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "#housing_labels=strat_train_set[\"median_house_value\"]\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "print(housing_prepared.shape)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(housing_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What is the output showing in the final cell?\n",
    "The tuples represent which element and the value on the right shows the element's value. e.g (0,0) 1.786 is the 0th array, 0th element, and it has value of 1.786"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
